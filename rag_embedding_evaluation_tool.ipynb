{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pcc9QMzFVbE"
      },
      "outputs": [],
      "source": [
        "#@title RAGãƒ‡ãƒ¼ã‚¿ç¢ºèªç”¨_æ¤œç´¢é †ä½è©•ä¾¡ãƒ„ãƒ¼ãƒ«\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "# ------------------------------------------------\n",
        "!pip install -q openai numpy scipy ipywidgets\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "import ipywidgets as widgets\n",
        "from google.colab import files\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, clear_output\n",
        "import re\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. APIã‚­ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–\n",
        "# ------------------------------------------------\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Colabã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆæ©Ÿèƒ½ã§'OPENAI_API_KEY'ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"âœ… OpenAI APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° (ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ç®¡ç†)\n",
        "# ------------------------------------------------\n",
        "documents = [] # å½¢å¼: [{\"id\": int, \"text\": str, \"metadata\": dict}, ...]\n",
        "doc_embeddings = None\n",
        "last_embedding_model = None\n",
        "last_division_settings = None\n",
        "file_name = \"uploaded_data\"\n",
        "current_query = \"\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4. UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å®šç¾©\n",
        "# ------------------------------------------------\n",
        "file_uploader = widgets.FileUpload(accept='.txt,.json', multiple=False, description='ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰')\n",
        "division_method_selector = widgets.Dropdown(\n",
        "    options=[('1è¡Œã‚’1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦åˆ†å‰²', 'line'), ('å›ºå®šé•·ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²', 'chunk'), ('JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«åˆ†å‰²', 'json_object')],\n",
        "    value='line', description='åˆ†å‰²æ–¹æ³•:', style={'description_width': 'initial'}\n",
        ")\n",
        "json_content_key_input = widgets.Text(value='content', placeholder='ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚­ãƒ¼å', description='JSONã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚­ãƒ¼:', layout=widgets.Layout(display='none'))\n",
        "chunk_size_input = widgets.IntText(value=500, description='ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º:', layout=widgets.Layout(display='none', width='200px'))\n",
        "chunk_overlap_input = widgets.IntText(value=50, description='ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—:', layout=widgets.Layout(display='none', width='200px'))\n",
        "division_settings_box = widgets.HBox([chunk_size_input, chunk_overlap_input, json_content_key_input])\n",
        "embedding_model_selector = widgets.Dropdown(\n",
        "    options=[('ä½¿ç”¨ã—ãªã„ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢)', 'none'), ('text-embedding-3-small', 'text-embedding-3-small'), ('text-embedding-3-large', 'text-embedding-3-large'), ('text-embedding-ada-002', 'text-embedding-ada-002')],\n",
        "    value='text-embedding-3-small', description='åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«:', style={'description_width': 'initial'}\n",
        ")\n",
        "query_input = widgets.Text(value='', placeholder='æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›...', description='ã‚¯ã‚¨ãƒª:', layout=widgets.Layout(width='80%'))\n",
        "search_button = widgets.Button(description='æ¤œç´¢å®Ÿè¡Œ', button_style='success', icon='search')\n",
        "output_area = widgets.Output()\n",
        "download_area = widgets.Output()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯)\n",
        "# ------------------------------------------------\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    if not text or not isinstance(text, str): return None\n",
        "    try:\n",
        "        return openai_client.embeddings.create(input=[text.replace(\"\\n\", \" \")], model=model).data[0].embedding\n",
        "    except Exception as e:\n",
        "        with output_area: print(f\"åŸ‹ã‚è¾¼ã¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size, chunk_overlap):\n",
        "    if chunk_size <= chunk_overlap: raise ValueError(\"ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚ˆã‚Šå¤§ããã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "    chunks, start_index = [], 0\n",
        "    while start_index < len(text):\n",
        "        end_index = start_index + chunk_size\n",
        "        chunks.append(text[start_index:end_index])\n",
        "        start_index += chunk_size - chunk_overlap\n",
        "    return chunks\n",
        "\n",
        "def update_documents(new_docs):\n",
        "    global documents, doc_embeddings, last_division_settings\n",
        "    documents = new_docs\n",
        "    doc_embeddings, last_division_settings = None, None\n",
        "    with output_area: print(f\"âœ… {len(documents)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚\")\n",
        "    display_source_document_downloader()\n",
        "\n",
        "def display_source_document_downloader():\n",
        "    with download_area:\n",
        "        clear_output()\n",
        "        if documents:\n",
        "            button = widgets.Button(description=f\"åˆ†å‰²ã•ã‚ŒãŸå…¨{len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\", button_style='info', icon='download')\n",
        "            def download_source(b):\n",
        "                base_name = os.path.splitext(file_name)[0]\n",
        "                dl_filename = f\"divided_{base_name}.txt\"\n",
        "                content = \"\\n\".join([f\"--- åˆ†å‰²ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ID: {d['id']} ---\\n{d.get('text', '')}\\n\" for d in documents])\n",
        "                with open(dl_filename, \"w\", encoding=\"utf-8\") as f: f.write(content)\n",
        "                files.download(dl_filename)\n",
        "            button.on_click(download_source)\n",
        "            display(widgets.VBox([widgets.HTML(\"<hr>\"), button]))\n",
        "\n",
        "def display_results_downloader(results, query):\n",
        "    \"\"\"â˜…å¾©å…ƒâ˜…: æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º\"\"\"\n",
        "    with download_area:\n",
        "        clear_output(wait=True)\n",
        "        display_source_document_downloader()\n",
        "        if results:\n",
        "            button = widgets.Button(description=f\"å…¨{len(results)}ä»¶ã®æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\", button_style='success', icon='download')\n",
        "            def download_csv(b):\n",
        "                query_sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"\", query)[:20]\n",
        "                dl_filename = f\"search_results_{query_sanitized}.csv\"\n",
        "                with open(dl_filename, 'w', newline='', encoding='utf-8-sig') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow(['Rank', 'Split_ID', 'Score', 'Text'])\n",
        "                    for i, r in enumerate(results):\n",
        "                        score_str = f\"{r['score']:.6f}\" if isinstance(r['score'], float) else str(r['score'])\n",
        "                        writer.writerow([i + 1, r['doc_info']['id'], score_str, r['doc_info']['text']])\n",
        "                files.download(dl_filename)\n",
        "            button.on_click(download_csv)\n",
        "            display(button)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6. ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ© (UIã®å‹•ä½œã‚’å®šç¾©)\n",
        "# ------------------------------------------------\n",
        "def on_division_method_change(change):\n",
        "    method = change['new']\n",
        "    chunk_size_input.layout.display = 'flex' if method == 'chunk' else 'none'\n",
        "    chunk_overlap_input.layout.display = 'flex' if method == 'chunk' else 'none'\n",
        "    json_content_key_input.layout.display = 'flex' if method == 'json_object' else 'none'\n",
        "\n",
        "def on_file_upload(change):\n",
        "    # (ã“ã®é–¢æ•°ã¯æ­£å¸¸ã«å‹•ä½œã—ã¾ã™)\n",
        "    global file_name\n",
        "    uploaded_file = change['new']\n",
        "    if not uploaded_file: return\n",
        "    file_info = next(iter(uploaded_file.values()))\n",
        "    file_name, content_bytes = file_info['metadata']['name'], file_info['content']\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(f\"'{file_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "        try:\n",
        "            new_docs = []\n",
        "            method = division_method_selector.value\n",
        "            if method == 'json_object':\n",
        "                if not file_name.endswith('.json'): raise ValueError(\"ã“ã®åˆ†å‰²æ–¹æ³•ã¯JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚\")\n",
        "                json_data = json.loads(content_bytes.decode('utf-8'))\n",
        "                if not isinstance(json_data, list): raise ValueError(\"JSONãƒ‡ãƒ¼ã‚¿ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "                if json_data and isinstance(json_data[0], dict):\n",
        "                    common_keys = ['content', 'text', 'body', 'document', 'description']\n",
        "                    found_key = next((key for key in common_keys if key in json_data[0]), None)\n",
        "                    if found_key:\n",
        "                        json_content_key_input.value = found_key\n",
        "                        print(f\"â„¹ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚­ãƒ¼ã¨ã—ã¦ '{found_key}' ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã—ãŸã€‚\")\n",
        "                content_key = json_content_key_input.value\n",
        "                for i, item in enumerate(json_data):\n",
        "                    if not isinstance(item, dict): continue\n",
        "                    metadata = item.get('metadata', {})\n",
        "                    content = item.get(content_key, \"\")\n",
        "                    meta_str = \", \".join([f\"{k}: {v}\" for k, v in metadata.items()])\n",
        "                    combined_text = f\"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: [ {meta_str} ]\\nå†…å®¹: {content}\"\n",
        "                    new_docs.append({\"id\": i, \"text\": combined_text, \"metadata\": metadata})\n",
        "            else:\n",
        "                full_text = content_bytes.decode('utf-8')\n",
        "                if method == 'chunk':\n",
        "                    chunks = split_text_into_chunks(full_text, chunk_size_input.value, chunk_overlap_input.value)\n",
        "                    new_docs = [{\"id\": i, \"text\": chunk, \"metadata\": {}} for i, chunk in enumerate(chunks)]\n",
        "                else:\n",
        "                    lines = full_text.splitlines()\n",
        "                    new_docs = [{\"id\": i, \"text\": line.strip(), \"metadata\": {}} for i, line in enumerate(lines) if line.strip()]\n",
        "            update_documents(new_docs)\n",
        "        except Exception as e:\n",
        "            with output_area: print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "            with download_area: clear_output()\n",
        "\n",
        "def on_search_button_clicked(b):\n",
        "    \"\"\"â˜…å¾©å…ƒâ˜…: æ¤œç´¢ãƒœã‚¿ãƒ³ãŒã‚¯ãƒªãƒƒã‚¯ã•ã‚ŒãŸã¨ãã®ãƒ¡ã‚¤ãƒ³å‡¦ç†\"\"\"\n",
        "    global documents, doc_embeddings, last_embedding_model, last_division_settings, current_query\n",
        "\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        current_query = query_input.value\n",
        "        embedding_model = embedding_model_selector.value\n",
        "        current_division_settings = (division_method_selector.value, chunk_size_input.value, chunk_overlap_input.value, json_content_key_input.value)\n",
        "\n",
        "        if not current_query: print(\"âŒ ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\"); return\n",
        "        if not documents: print(\"âŒ æ¤œç´¢å¯¾è±¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\"); return\n",
        "\n",
        "        print(f\"ğŸ” æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...\\nã‚¯ã‚¨ãƒª: {current_query}\\nåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {embedding_model}\\n\" + \"-\" * 30)\n",
        "\n",
        "        results = []\n",
        "        doc_texts = [d['text'] for d in documents]\n",
        "\n",
        "        if embedding_model == 'none':\n",
        "            print(\"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...\")\n",
        "            query_words = current_query.lower().split()\n",
        "            for doc_info in documents:\n",
        "                score = sum(1 for word in query_words if word in doc_info['text'].lower())\n",
        "                results.append({'doc_info': doc_info, 'score': score})\n",
        "            results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        else:\n",
        "            if doc_embeddings is None or last_embedding_model != embedding_model or last_division_settings != current_division_settings:\n",
        "                print(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­ (ãƒ¢ãƒ‡ãƒ«: {embedding_model})...\")\n",
        "                temp_embeddings = [get_embedding(text, embedding_model) for text in doc_texts]\n",
        "\n",
        "                valid_docs_with_embeddings = [(documents[i], emb) for i, emb in enumerate(temp_embeddings) if emb is not None]\n",
        "                if not valid_docs_with_embeddings: print(\"âŒ ã™ã¹ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚\"); return\n",
        "\n",
        "                valid_docs, doc_embeddings_list = zip(*valid_docs_with_embeddings)\n",
        "                documents = list(valid_docs)\n",
        "                doc_embeddings = np.array(doc_embeddings_list)\n",
        "                last_embedding_model = embedding_model\n",
        "                last_division_settings = current_division_settings\n",
        "                print(\"âœ… åŸ‹ã‚è¾¼ã¿å®Œäº†ã€‚\")\n",
        "\n",
        "            print(\"ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆä¸­...\")\n",
        "            query_embedding = get_embedding(current_query, embedding_model)\n",
        "            if query_embedding is None: print(\"âŒ ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\"); return\n",
        "\n",
        "            print(\"é¡ä¼¼åº¦ã‚’è¨ˆç®—ä¸­...\")\n",
        "            similarities = [1 - cosine(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
        "            sorted_indices = np.argsort(similarities)[::-1]\n",
        "            results = [{'doc_info': documents[i], 'score': similarities[i]} for i in sorted_indices]\n",
        "\n",
        "        print(f\"\\nğŸ† æ¤œç´¢çµæœ (å…¨{len(results)}ä»¶) ğŸ†\\n\")\n",
        "        if not results:\n",
        "            print(\"ä¸€è‡´ã™ã‚‹çµæœã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "        else:\n",
        "            for i, result in enumerate(results):\n",
        "                score_str = f\"{result['score']:.4f}\" if isinstance(result['score'], float) else str(result['score'])\n",
        "                print(f\"ã€Rank {i+1}ã€‘(åˆ†å‰²ID: {result['doc_info']['id']}) Score: {score_str}\")\n",
        "                print(f\"   ğŸ“„ {result['doc_info']['text']}\")\n",
        "                print(\"-\" * 20)\n",
        "\n",
        "    display_results_downloader(results, current_query)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7. UIã®æœ€çµ‚çš„ãªçµ„ã¿ç«‹ã¦ã¨ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã®ç™»éŒ²\n",
        "# ------------------------------------------------\n",
        "division_method_selector.observe(on_division_method_change, names='value')\n",
        "file_uploader.observe(on_file_upload, names='value')\n",
        "search_button.on_click(on_search_button_clicked)\n",
        "on_division_method_change({'new': division_method_selector.value})\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h2>æ¤œç´¢é †ä½ è©•ä¾¡ãƒ„ãƒ¼ãƒ«</h2>\"),\n",
        "    widgets.HTML(\"\"\"\n",
        "    <p><b>ä½¿ã„æ–¹:</b></p>\n",
        "    <ol>\n",
        "      <li><b>åˆ†å‰²æ–¹æ³•</b>ã‚’é¸æŠã—ã¾ã™ã€‚ï¼ˆJSONã®å ´åˆã¯ã€ŒJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã€ã‚’é¸ã¶ã¨ä¾¿åˆ©ã§ã™ï¼‰</li>\n",
        "      <li><b>ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰</b>ã—ã¾ã™ã€‚JSONã®å ´åˆã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚­ãƒ¼ãŒè‡ªå‹•ã§æ¨æ¸¬ã•ã‚Œã¾ã™ã€‚</li>\n",
        "      <li>ï¼ˆä»»æ„ï¼‰åˆ†å‰²ã•ã‚ŒãŸå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€æ„å›³é€šã‚Šã‹ç¢ºèªã—ã¾ã™ã€‚</li>\n",
        "      <li><b>ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›</b>ã—ã¦ã€Œæ¤œç´¢å®Ÿè¡Œã€ã™ã‚‹ã¨ã€å…¨ä»¶ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒè¡¨ç¤ºã•ã‚Œã€çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚</li>\n",
        "    </ol>\n",
        "    \"\"\"),\n",
        "    widgets.HBox([file_uploader, division_method_selector]),\n",
        "    division_settings_box,\n",
        "    embedding_model_selector,\n",
        "    widgets.HBox([query_input, search_button]),\n",
        "    widgets.HTML(\"<hr>\"),\n",
        "    output_area,\n",
        "    download_area\n",
        "])\n",
        "\n",
        "display(ui)"
      ]
    }
  ]
}